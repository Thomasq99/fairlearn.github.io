
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Fairness in Machine Learning &#8212; Fairlearn 0.4.7.dev0 documentation</title>
    
  <link rel="stylesheet" href="../_static/css/index.d431a4ee1c1efae0e38bdfebc22debff.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.30270b6e4c972e43c488.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"E": "{\\mathbb{E}}", "P": "{\\mathbb{P}}", "given": "\\mathbin{\\vert}"}}})</script>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Assessment" href="assessment.html" />
    <link rel="prev" title="User Guide" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    
    <a class="navbar-brand" href="../contents.html">
      <img src="../_static/fairlearn_full_color.png" class="logo" alt="logo">
    </a>
    
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="../about/index.html">About</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../quickstart.html">Quickstart</a>
        </li>
        
        <li class="nav-item active">
            <a class="nav-link" href="index.html">User Guide</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../api_reference/index.html">API Reference</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../auto_examples/index.html">Example Notebooks</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../contributor_guide/index.html">Contributor Guide</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="../faq.html">FAQ</a>
        </li>
        
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://gitter.im/fairlearn/community">Gitter<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://stackoverflow.com/questions/tagged/fairlearn">StackOverflow<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>


      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/fairlearn/fairlearn" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar">

<form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>


<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

  <div class="bd-toc-item active">
  

  <ul class="nav bd-sidenav">
      
      
      
      
      
      
        
          
              <li class="active">
                  <a href="">Fairness in Machine Learning</a>
              </li>
          
        
          
              <li class="">
                  <a href="assessment.html">Assessment</a>
              </li>
          
        
          
              <li class="">
                  <a href="mitigation.html">Mitigation</a>
              </li>
          
        
          
              <li class="">
                  <a href="further_resources.html">Further Resources</a>
              </li>
          
        
      
      
      
      
      
      
      
      
      
      
    </ul>

</nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#fairness-of-ai-systems" class="nav-link">Fairness of AI systems</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#types-of-harms" class="nav-link">Types of harms</a>
        </li>
    
        <li class="nav-item toc-entry toc-h2">
            <a href="#fairness-assessment-and-unfairness-mitigation" class="nav-link">Fairness assessment and unfairness mitigation</a><ul class="nav section-nav flex-column">
                
        <li class="nav-item toc-entry toc-h3">
            <a href="#group-fairness-sensitive-features" class="nav-link">Group fairness, sensitive features</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#parity-constraints" class="nav-link">Parity constraints</a>
        </li>
    
        <li class="nav-item toc-entry toc-h3">
            <a href="#disparity-metrics-group-metrics" class="nav-link">Disparity metrics, group metrics</a>
        </li>
    
            </ul>
        </li>
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="terminology">
<span id="fairness-in-machine-learning"></span><span id="id1"></span><h1><span class="section-number">1. </span>Fairness in Machine Learning<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h1>
<div class="section" id="fairness-of-ai-systems">
<h2><span class="section-number">1.1. </span>Fairness of AI systems<a class="headerlink" href="#fairness-of-ai-systems" title="Permalink to this headline">¶</a></h2>
<p>AI systems can behave unfairly for a variety of reasons. Sometimes it is
because of societal biases reflected in the training data and in the decisions
made during the development and deployment of these systems. In other cases,
AI systems behave unfairly not because of societal biases, but because of
characteristics of the data (e.g., too few data points about some group of
people) or characteristics of the systems themselves. It can be hard to
distinguish between these reasons, especially since they are not mutually
exclusive and often exacerbate one another. Therefore, we define whether an AI
system is behaving unfairly in terms of its impact on people — i.e., in terms
of harms — and not in terms of specific causes, such as societal biases, or in
terms of intent, such as prejudice.</p>
<p><strong>Usage of the word bias.</strong> Since we define fairness in terms of harms
rather than specific causes (such as societal biases), we avoid the usage of
the words <em>bias</em> or <em>debiasing</em> in describing the functionality of Fairlearn.</p>
</div>
<div class="section" id="types-of-harms">
<h2><span class="section-number">1.2. </span>Types of harms<a class="headerlink" href="#types-of-harms" title="Permalink to this headline">¶</a></h2>
<p>There are many types of harms (see, e.g., the
<a class="reference external" href="https://www.youtube.com/watch?v=fMym_BKWQzk">keynote by K. Crawford at NeurIPS 2017</a>).
The Fairlearn package is most applicable to two kinds of harms:</p>
<ul class="simple">
<li><p><em>Allocation harms</em> can occur when AI systems extend or withhold
opportunities, resources, or information. Some of the key applications are in
hiring, school admissions, and lending.</p></li>
<li><p><em>Quality-of-service harms</em> can occur when a system does not work as well for
one person as it does for another, even if no opportunities, resources, or
information are extended or withheld. Examples include varying accuracy in
face recognition, document search, or product recommendation.</p></li>
</ul>
</div>
<div class="section" id="fairness-assessment-and-unfairness-mitigation">
<h2><span class="section-number">1.3. </span>Fairness assessment and unfairness mitigation<a class="headerlink" href="#fairness-assessment-and-unfairness-mitigation" title="Permalink to this headline">¶</a></h2>
<p>In Fairlearn, we provide tools to assess fairness of predictors for
classification and regression. We also provide tools that mitigate unfairness
in classification and regression. In both assessment and mitigation scenarios,
fairness is quantified using disparity metrics as we describe below.</p>
<div class="section" id="group-fairness-sensitive-features">
<h3><span class="section-number">1.3.1. </span>Group fairness, sensitive features<a class="headerlink" href="#group-fairness-sensitive-features" title="Permalink to this headline">¶</a></h3>
<p>There are many approaches to conceptualizing fairness. In Fairlearn, we follow
the approach known as group fairness, which asks: <em>Which groups of individuals
are at risk for experiencing harms?</em></p>
<p>The relevant groups (also called subpopulations) are defined using <strong>sensitive
features</strong> (or sensitive attributes), which are passed to a Fairlearn
estimator as a vector or a matrix called <code class="code docutils literal notranslate"><span class="pre">sensitive_features</span></code> (even if it is
only one feature). The term suggests that the system designer should be
sensitive to these features when assessing group fairness. Although these
features may sometimes have privacy implications (e.g., gender or age) in
other cases they may not (e.g., whether or not someone is a native speaker of
a particular language). Moreover, the word sensitive does not imply that
these features should not be used to make predictions – indeed, in some cases
it may be better to include them.</p>
<p>Fairness literature also uses the term <em>protected attribute</em> in a similar
sense as sensitive feature. The term is based on anti-discrimination laws
that define specific <em>protected classes</em>. Since we seek to apply group
fairness in a wider range of settings, we avoid this term.</p>
</div>
<div class="section" id="parity-constraints">
<h3><span class="section-number">1.3.2. </span>Parity constraints<a class="headerlink" href="#parity-constraints" title="Permalink to this headline">¶</a></h3>
<p>Group fairness is typically formalized by a set of constraints on the behavior
of the predictor called <strong>parity constraints</strong> (also called criteria). Parity
constraints require that some aspect (or aspects) of the predictor behavior be
comparable across the groups defined by sensitive features.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> denote a feature vector used for predictions, <span class="math notranslate nohighlight">\(A\)</span> be a
single sensitive feature (such as age or race), and <span class="math notranslate nohighlight">\(Y\)</span> be the true
label. Parity constraints are phrased in terms of expectations with respect to
the distribution over <span class="math notranslate nohighlight">\((X,A,Y)\)</span>.
For example, in Fairlearn, we consider the following types of parity constraints.</p>
<p><em>Binary classification</em>:</p>
<ul class="simple">
<li><p><em>Demographic parity</em> (also known as <em>statistical parity</em>): A classifier
<span class="math notranslate nohighlight">\(h\)</span> satisfies demographic parity under a distribution over
<span class="math notranslate nohighlight">\((X, A, Y)\)</span> if its prediction <span class="math notranslate nohighlight">\(h(X)\)</span> is statistically
independent of the sensitive feature <span class="math notranslate nohighlight">\(A\)</span>. This is equivalent to
<span class="math notranslate nohighlight">\(\E[h(X) \given A=a] = \E[h(X)] \quad \forall a\)</span>. <a class="footnote-reference brackets" href="#id9" id="id2">6</a></p></li>
<li><p><em>Equalized odds</em>: A classifier <span class="math notranslate nohighlight">\(h\)</span> satisfies equalized odds under a
distribution over <span class="math notranslate nohighlight">\((X, A, Y)\)</span> if its prediction <span class="math notranslate nohighlight">\(h(X)\)</span> is
conditionally independent of the sensitive feature <span class="math notranslate nohighlight">\(A\)</span> given the label
<span class="math notranslate nohighlight">\(Y\)</span>. This is equivalent to
<span class="math notranslate nohighlight">\(\E[h(X) \given A=a, Y=y] = \E[h(X) \given Y=y] \quad \forall a, y\)</span>.
<a class="footnote-reference brackets" href="#id9" id="id3">6</a></p></li>
<li><p><em>Equal opportunity</em>: a relaxed version of equalized odds that only considers
conditional expectations with respect to positive labels, i.e., <span class="math notranslate nohighlight">\(Y=1\)</span>.
<a class="footnote-reference brackets" href="#id8" id="id4">5</a></p></li>
</ul>
<p><em>Regression</em>:</p>
<ul class="simple">
<li><p><em>Demographic parity</em>: A predictor <span class="math notranslate nohighlight">\(f\)</span> satisfies demographic parity
under a distribution over <span class="math notranslate nohighlight">\((X, A, Y)\)</span> if <span class="math notranslate nohighlight">\(f(X)\)</span> is independent
of the sensitive feature <span class="math notranslate nohighlight">\(A\)</span>. This is equivalent to
<span class="math notranslate nohighlight">\(\P[f(X) \geq z \given A=a] = \P[f(X) \geq z] \quad \forall a, z\)</span>.
<a class="footnote-reference brackets" href="#id7" id="id5">4</a></p></li>
<li><p><em>Bounded group loss</em>: A predictor <span class="math notranslate nohighlight">\(f\)</span> satisfies bounded group loss at
level <span class="math notranslate nohighlight">\(\zeta\)</span> under a distribution over <span class="math notranslate nohighlight">\((X, A, Y)\)</span> if
<span class="math notranslate nohighlight">\(\E[loss(Y, f(X)) \given A=a] \leq \zeta \quad \forall a\)</span>. <a class="footnote-reference brackets" href="#id7" id="id6">4</a></p></li>
</ul>
<p>Above, demographic parity seeks to mitigate allocation harms, whereas bounded
group loss primarily seeks to mitigate quality-of-service harms. Equalized
odds and equal opportunity can be used as a diagnostic for both allocation
harms as well as quality-of-service harms.</p>
</div>
<div class="section" id="disparity-metrics-group-metrics">
<h3><span class="section-number">1.3.3. </span>Disparity metrics, group metrics<a class="headerlink" href="#disparity-metrics-group-metrics" title="Permalink to this headline">¶</a></h3>
<p>Disparity metrics evaluate how far a given predictor departs from satisfying a
parity constraint. They can either compare the behavior across different
groups in terms of ratios or in terms of differences. For example, for binary
classification:</p>
<ul class="simple">
<li><p><em>Demographic parity difference</em> is defined as
<span class="math notranslate nohighlight">\((\max_a \E[h(X) \given A=a]) - (\min_a \E[h(X) \given A=a])\)</span>.</p></li>
<li><p><em>Demographic parity ratio</em> is defined as
<span class="math notranslate nohighlight">\(\dfrac{\min_a \E[h(X) \given A=a]}{\max_a \E[h(X) \given A=a]}\)</span>.</p></li>
</ul>
<p>The Fairlearn package provides the functionality to convert common accuracy
and error metrics from <cite>scikit-learn</cite> to <em>group metrics</em>, i.e., metrics that
are evaluated on the entire data set and also on each group individually.
Additionally, group metrics yield the minimum and maximum metric value and for
which groups these values were observed, as well as the difference and ratio
between the maximum and the minimum values. For more information refer to the
subpackage <code class="code docutils literal notranslate"><span class="pre">fairlearn.metrics</span></code>.</p>
<div class="topic">
<p class="topic-title">References:</p>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets">4</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Agarwal, Dudik, Wu <a class="reference external" href="https://arxiv.org/pdf/1905.12843.pdf">“Fair Regression: Quantitative Definitions and
Reduction-based Algorithms”</a>,
ICML, 2019.</p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Hardt, Price, Srebro <a class="reference external" href="https://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">“Equality of Opportunity in Supervised
Learning”</a>,
NIPS, 2016.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">6</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id3">2</a>)</span></dt>
<dd><p>Agarwal, Beygelzimer, Dudik, Langford, Wallach <a class="reference external" href="https://arxiv.org/pdf/1803.02453.pdf">“A Reductions
Approach to Fair Classification”</a>, ICML, 2018.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.30270b6e4c972e43c488.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2019, Microsoft Corporation and contributors..<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>